{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf7db10",
   "metadata": {},
   "source": [
    "# **Final Project - Deep Active Learning for Camera Trap Image Classification**\n",
    "## **Course: Quantitative Methods: Conservation Biology**\n",
    "## **Author: David Lezama**\n",
    "## **Date: December 2025**\n",
    "\n",
    "_ _ _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e180f",
   "metadata": {},
   "source": [
    "# **Introduction and Paper Selection** \n",
    "\n",
    "\n",
    "Camera traps are essential tools in conservation biology. They produce enormous numbers of images that document wildlife presence, behavior, habitat use, and population trends. The major challenge, however, is that manually labeling these images is slow, inconsistent, and often too resource-intensive for conservation teams to process at the scale needed to support ecological decision-making. As the number of camera trap deployments increases worldwide, the need for automated and scalable image classification methods becomes even more important.\n",
    "\n",
    "The goal of this project is to replicate, adapt, and apply a deep active learning pipeline for species identification in camera-trap images, using a smaller real-world dataset and a fully self-trained embedding model. The central goal is to determine whether active learning, where the model selects the most informative images to label, can reduce human labeling effort while still achieving high classification accuracy. In other words, the project asks:\n",
    "**Can deep active learning make wildlife monitoring faster, more accurate, and more efficient, even when only limited labeled data are available?**\n",
    "\n",
    "This project is based on the research article: “**A deep active learning system for species identification and counting in camera trap images.**”\n",
    "\n",
    "**Link:** https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13504\n",
    "\n",
    "The paper introduces a scalable pipeline combining object detection, transfer learning, embedding models, and multiple active learning strategies to dramatically reduce labeling requirements, achieving state-of-the-art accuracy while using 99.5% fewer manual labels.\n",
    "\n",
    "## **How This Project Connects to the Original Article**\n",
    "My project directly implements the structure of the authors' pipeline, including:\n",
    "\n",
    "• active_learning_methods/ (uncertainty, diversity, hybrid, and clustering-based samplers)\n",
    "\n",
    "\n",
    "• deep_learning/ (embedding learning, triplet loss, data loaders, training loops, and active learning manager)\n",
    "\n",
    "\n",
    "• Adapted versions of the authors’ scripts:\n",
    "\n",
    "\n",
    "    • train_embedding.py\n",
    "\n",
    "\n",
    "    • run_active_learning.py\n",
    "\n",
    "\n",
    "However, the pretrained embedding model used in the paper, critical for initializing the active learning process, was no longer available from the authors' Google Drive. Because of this, I had to:\n",
    "\n",
    "• Train a new embedding model from scratch using ResNet-18 with triplet loss\n",
    "\n",
    "\n",
    "• Modify environment settings and dependencies\n",
    "\n",
    "\n",
    "• Write a custom active learning script (run_my_camera_trap_active_learning.py)\n",
    "\n",
    "\n",
    "• Adjust hyperparameters (balanced_P, balanced_K, sampler settings)\n",
    "\n",
    "\n",
    "• Solve issues related to class imbalance and insufficient sample sizes\n",
    "\n",
    "\n",
    "This means the project is not just a reproduction, but a functional reconstruction of the paper’s pipeline using new data and independently trained models.\n",
    "\n",
    "## **Why This Matters for Conservation Biology**\n",
    "\n",
    "By applying this pipeline to a smaller camera-trap dataset, this project demonstrates how:\n",
    "\n",
    "1. Deep embeddings capture meaningful differences between species, even with limited training data.\n",
    "\n",
    "\n",
    "2. Active learning reduces labeling effort by allowing the model to request only the most informative images.\n",
    "\n",
    "\n",
    "3. These techniques together accelerate biodiversity monitoring, enabling conservation teams to process large image datasets without requiring millions of manual labels.\n",
    "\n",
    "\n",
    "This directly supports the goals of Quantitative Methods: Conservation Biology, which emphasizes reproducible modeling, efficient data use, and the development of computational tools that support ecological decision-making.\n",
    "\n",
    "## **Dataset Used in This Project:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd9aca2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species/Class</th>\n",
       "      <th>Number of Images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deer</td>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fox</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Raccoon</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Squirrel</td>\n",
       "      <td>1121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Total</td>\n",
       "      <td>3212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Species/Class  Number of Images\n",
       "0          Deer               744\n",
       "1           Fox               349\n",
       "2       Raccoon               196\n",
       "3      Squirrel              1121\n",
       "4    Background               802\n",
       "5         Total              3212"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TABLE 1 — Species Counts\n",
    "\n",
    "species_table = pd.DataFrame({\n",
    "    \"Species/Class\": [\"Deer\", \"Fox\", \"Raccoon\", \"Squirrel\", \"Background\", \"Total\"],\n",
    "    \"Number of Images\": [744, 349, 196, 1121, 802, 3212]\n",
    "})\n",
    "\n",
    "display(species_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60587558",
   "metadata": {},
   "source": [
    "I added background images intentionally because real camera-trap deployments capture far more empty frames than wildlife. Including a background class prevents false positives and is necessary for realistic ecological applications.\n",
    "\n",
    "Goal and outcome: This project uses a ResNet-18 embedding model trained with triplet loss together with a margin-based uncertainty sampler in an active learning pipeline. The system begins with one thousand labeled images and adds one hundred newly labeled images at each iteration, generating evaluation points at 1000, 1100, 1200, 1300, and continuing in one-hundred-image increments up to 2500 labeled images. At each of these labeled-image counts, the active learning manager extracts embeddings, retrains the classifier, and produces a numerical test accuracy value. These accuracy values form the accuracy-progression table that shows how the ResNet-18–based active learning system improves as additional informative images are selected and labeled.\n",
    "\n",
    "_ _ _\n",
    "\n",
    "# **Data Collection and Preprocessing**\n",
    "\n",
    "## **Raw Data Documentation**\n",
    "\n",
    "The dataset consists of image:\n",
    "\n",
    "Camera_trap_images/\n",
    "   - Deer/\n",
    "   - Fox/\n",
    "   - Raccoon/\n",
    "   - Squirrel/\n",
    "   - Background/\n",
    "\n",
    "Each folder contains JPEG images already labeled by folder name.\n",
    "\n",
    "## **Data Cleaning Explanation**\n",
    "\n",
    "Although the images were already labeled by folder, I performed the following cleaning steps:\n",
    "\n",
    "1. Verified that all image files could be opened (filtered out corrupted images).\n",
    "\n",
    "\n",
    "2. Standardized all image file formats to.jpg for consistent loading.\n",
    "\n",
    "\n",
    "3. Removed duplicate images using file hashing.\n",
    "\n",
    "\n",
    "4. Ensured every image had an associated class folder, preventing unlabeled items from entering the model.\n",
    "\n",
    "\n",
    "These steps ensure that downstream training does not fail due to missing or unreadable files.\n",
    "\n",
    "## **Preprocessing Steps**\n",
    "\n",
    "• Object detection (from the paper) handles cropping, but since I trained embeddings myself, the images were directly passed into the ResNet18 embedding model.\n",
    "\n",
    "\n",
    "• All images were resized to 256×256 as expected by the embedding network.\n",
    "\n",
    "\n",
    "• Triplet-loss batch sampler required balancing parameters P (classes per batch) and K (samples per class).\n",
    "\n",
    "\n",
    "## **Data Visualization (Histogram of Class Distribution)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9fa717c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = [\"Deer\",\"Fox\",\"Raccoon\",\"Squirrel\",\"Background\"]\n",
    "counts = [744,349,196,1121,802]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(classes, counts)\n",
    "plt.title(\"Class Distribution in Camera Trap Dataset\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ad7b4",
   "metadata": {},
   "source": [
    "## **Data Setup for Modeling**\n",
    "\n",
    "• Images are organized into labeled folders for embedding training.\n",
    "\n",
    "\n",
    "• During active learning, labeled and unlabeled lists are tracked inside snapshot files.\n",
    "\n",
    "\n",
    "• Output of embedding model → 256-dimensional feature vectors.\n",
    "\n",
    "\n",
    "• Active learning sampler selects the next 100 queries per iteration.\n",
    "\n",
    "_ _ _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8122917",
   "metadata": {},
   "source": [
    "# **Full Repository Structure and File Documentation** \n",
    "\n",
    "## **Folders Imported From the Research Paper**\n",
    "\n",
    "### **active_learning_methods/**\n",
    "\n",
    "This folder contains all active learning algorithms. It includes:\n",
    "\n",
    "**utils/**\n",
    "\n",
    "• tree.py:  hierarchical clustering tree utilities\n",
    "\n",
    "\n",
    "• Tree_test.py: tests for the clustering implementation\n",
    "\n",
    "• __init__.py\n",
    "\n",
    "\n",
    "**Main active learning modules**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a04fdd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>margin_AL.py</td>\n",
       "      <td>Margin uncertainty sampling (strategy used in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entropy_sampling.py</td>\n",
       "      <td>Entropy-based uncertainty selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confidence_sampling.py</td>\n",
       "      <td>Low-confidence sampling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>graph_density.py</td>\n",
       "      <td>Diversity-based sampling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kcenter_greedy.py</td>\n",
       "      <td>Core-set sampling (diversity)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>informative_diverse.py</td>\n",
       "      <td>Hybrid informative–diverse sampler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hierarchical_clustering_AL.py</td>\n",
       "      <td>Cluster-based sampling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mixture_of_samplers.py</td>\n",
       "      <td>Weighted combination of strategies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>simulate_batch.py</td>\n",
       "      <td>Simulated sampling evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>represent_cluster_centers.py</td>\n",
       "      <td>Sampling based on cluster center representation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bandit_discrete.py</td>\n",
       "      <td>Multi-armed bandit sampler for adaptive AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>uniform_sampling.py</td>\n",
       "      <td>Random baseline sampling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sampling_def.py</td>\n",
       "      <td>Abstract sampler class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wrapper_sampler_def.py</td>\n",
       "      <td>Sampler wrapper class and registry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>constants.py</td>\n",
       "      <td>Shared configuration for samplers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             File  \\\n",
       "0                    margin_AL.py   \n",
       "1             entropy_sampling.py   \n",
       "2          confidence_sampling.py   \n",
       "3                graph_density.py   \n",
       "4               kcenter_greedy.py   \n",
       "5          informative_diverse.py   \n",
       "6   hierarchical_clustering_AL.py   \n",
       "7          mixture_of_samplers.py   \n",
       "8               simulate_batch.py   \n",
       "9    represent_cluster_centers.py   \n",
       "10             bandit_discrete.py   \n",
       "11            uniform_sampling.py   \n",
       "12                sampling_def.py   \n",
       "13         wrapper_sampler_def.py   \n",
       "14                   constants.py   \n",
       "\n",
       "                                              Purpose  \n",
       "0   Margin uncertainty sampling (strategy used in ...  \n",
       "1                 Entropy-based uncertainty selection  \n",
       "2                             Low-confidence sampling  \n",
       "3                            Diversity-based sampling  \n",
       "4                       Core-set sampling (diversity)  \n",
       "5                  Hybrid informative–diverse sampler  \n",
       "6                              Cluster-based sampling  \n",
       "7                  Weighted combination of strategies  \n",
       "8                       Simulated sampling evaluation  \n",
       "9     Sampling based on cluster center representation  \n",
       "10         Multi-armed bandit sampler for adaptive AL  \n",
       "11                           Random baseline sampling  \n",
       "12                             Abstract sampler class  \n",
       "13                 Sampler wrapper class and registry  \n",
       "14                  Shared configuration for samplers  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TABLE 2 — Active Learning Methods\n",
    "\n",
    "al_methods_table = pd.DataFrame({\n",
    "    \"File\": [\n",
    "        \"margin_AL.py\", \"entropy_sampling.py\", \"confidence_sampling.py\",\n",
    "        \"graph_density.py\", \"kcenter_greedy.py\", \"informative_diverse.py\",\n",
    "        \"hierarchical_clustering_AL.py\", \"mixture_of_samplers.py\",\n",
    "        \"simulate_batch.py\", \"represent_cluster_centers.py\",\n",
    "        \"bandit_discrete.py\", \"uniform_sampling.py\", \"sampling_def.py\",\n",
    "        \"wrapper_sampler_def.py\", \"constants.py\"\n",
    "    ],\n",
    "    \"Purpose\": [\n",
    "        \"Margin uncertainty sampling (strategy used in my project)\",\n",
    "        \"Entropy-based uncertainty selection\",\n",
    "        \"Low-confidence sampling\",\n",
    "        \"Diversity-based sampling\",\n",
    "        \"Core-set sampling (diversity)\",\n",
    "        \"Hybrid informative–diverse sampler\",\n",
    "        \"Cluster-based sampling\",\n",
    "        \"Weighted combination of strategies\",\n",
    "        \"Simulated sampling evaluation\",\n",
    "        \"Sampling based on cluster center representation\",\n",
    "        \"Multi-armed bandit sampler for adaptive AL\",\n",
    "        \"Random baseline sampling\",\n",
    "        \"Abstract sampler class\",\n",
    "        \"Sampler wrapper class and registry\",\n",
    "        \"Shared configuration for samplers\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(al_methods_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584205f3",
   "metadata": {},
   "source": [
    "This folder gives the pipeline significant flexibility, but my project specifically used margin-based sampling, which selects images where the classifier is most uncertain.\n",
    "\n",
    "### **deep_learning/**\n",
    "\n",
    "Contains all deep-learning utilities used in training the embedding model and running active learning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c80e6254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>active_learning_manager.py</td>\n",
       "      <td>Runs embedding updates, classifier retraining,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_loader.py</td>\n",
       "      <td>Loads images, computes dataset statistics, app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>engine.py</td>\n",
       "      <td>Training loops, validation routines, embedding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>losses.py</td>\n",
       "      <td>Triplet loss and contrastive loss implementations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>networks.py</td>\n",
       "      <td>ResNet18 embedding backbone + classification head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>utils.py</td>\n",
       "      <td>Checkpointing, metrics, visualization utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>__init__.py</td>\n",
       "      <td>Package initialization</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         File  \\\n",
       "0  active_learning_manager.py   \n",
       "1              data_loader.py   \n",
       "2                   engine.py   \n",
       "3                   losses.py   \n",
       "4                 networks.py   \n",
       "5                    utils.py   \n",
       "6                 __init__.py   \n",
       "\n",
       "                                            Function  \n",
       "0  Runs embedding updates, classifier retraining,...  \n",
       "1  Loads images, computes dataset statistics, app...  \n",
       "2  Training loops, validation routines, embedding...  \n",
       "3  Triplet loss and contrastive loss implementations  \n",
       "4  ResNet18 embedding backbone + classification head  \n",
       "5    Checkpointing, metrics, visualization utilities  \n",
       "6                             Package initialization  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TABLE 3 — deep_learning Folder Functions\n",
    "\n",
    "\n",
    "deep_learning_table = pd.DataFrame({\n",
    "    \"File\": [\n",
    "        \"active_learning_manager.py\", \"data_loader.py\", \"engine.py\",\n",
    "        \"losses.py\", \"networks.py\", \"utils.py\", \"__init__.py\"\n",
    "    ],\n",
    "    \"Function\": [\n",
    "        \"Runs embedding updates, classifier retraining, and AL batch selection\",\n",
    "        \"Loads images, computes dataset statistics, applies transforms\",\n",
    "        \"Training loops, validation routines, embedding extraction\",\n",
    "        \"Triplet loss and contrastive loss implementations\",\n",
    "        \"ResNet18 embedding backbone + classification head\",\n",
    "        \"Checkpointing, metrics, visualization utilities\",\n",
    "        \"Package initialization\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(deep_learning_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0e1d5",
   "metadata": {},
   "source": [
    "### **Scripts Taken From the Repository**\n",
    "\n",
    "• train_embedding.py — trains a new embedding model\n",
    "\n",
    "\n",
    "• run_active_learning.py — runs the active learning pipeline\n",
    "\n",
    "\n",
    "However, these scripts required modifications to work with my dataset, so I created:\n",
    "\n",
    "### **My Custom Script**\n",
    "\n",
    "• run_my_camera_trap_active_learning.py\n",
    "\n",
    " This file contains fixes for sampling issues, adjusted arguments, and a corrected train/test evaluation system.\n",
    "\n",
    " **_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "## **Files Produced During the Project**\n",
    "\n",
    "### **Embedding Model Checkpoints**\n",
    "\n",
    "Training for 3 epochs produced:\n",
    "\n",
    "• camera_trap_triplet_resnet18_0001.tar\n",
    "\n",
    "\n",
    "• camera_trap_triplet_resnet18_0002.tar\n",
    "\n",
    "\n",
    "• camera_trap_triplet_resnet18_0003.tar\n",
    "\n",
    "\n",
    "These checkpoints contain the embedding weights but do not produce visual output on their own, which is expected.\n",
    "\n",
    "\n",
    "### **Visualization Outputs**\n",
    "\n",
    "The following were also generated:\n",
    "\n",
    "• camera_trap__train_1.jpg\n",
    "\n",
    "\n",
    "• camera_trap__train_2.jpg\n",
    "\n",
    "\n",
    "• camera_trap__train_3.jpg\n",
    "\n",
    "\n",
    "• camera_trap__val_1.jpg\n",
    "\n",
    "\n",
    "• camera_trap__val_2.jpg\n",
    "\n",
    "\n",
    "• camera_trap__val_3.jpg\n",
    "\n",
    "\n",
    "These show Dim1 vs Dim2 embeddings, giving a 2D projection of the feature space.\n",
    "\n",
    "They appear empty or uninformative early in training because:\n",
    "\n",
    "• Embeddings after only a few epochs are not yet well-separated\n",
    "\n",
    "\n",
    "• Visualization uses a small fixed batch\n",
    "\n",
    "\n",
    "• Triplet loss needs more negative/positive pairs to form tight clusters\n",
    "\n",
    "\n",
    "This behavior is normal and described in the paper as well.\n",
    "\n",
    "### **Active Learning Snapshot Files**\n",
    "\n",
    "Running AL produced:\n",
    "\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "AL_snapshot_1000.pth  \n",
    "AL_snapshot_1100.pth  \n",
    "...\n",
    "AL_snapshot_3200.pth  \n",
    "finetuned_1000.tar  \n",
    "finetuned_2000.tar\n",
    "\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "Snapshots store:\n",
    "\n",
    "• Current embedding model\n",
    "\n",
    "\n",
    "• Current classifier weights\n",
    "\n",
    "\n",
    "• Which images have been labeled\n",
    "\n",
    "\n",
    "• Sampler state\n",
    "\n",
    "\n",
    "They are not viewable images; they are checkpoints used for restoring a previous AL state.\n",
    "_ _ _\n",
    "\n",
    "# **Environment Setup and Terminal Commands**\n",
    "\n",
    "I used an isolated environment named sklearn-env to avoid dependency conflicts. Also, this is all done in a new bash terminal.\n",
    "\n",
    "## **Creating and Using the Environment**\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "conda activate sklearn-env\n",
    "\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "\n",
    "## **Installing Required Packages**\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "pip install scikit-learn\n",
    "pip install numpy\n",
    "pip install scipy\n",
    "pip install pillow\n",
    "pip install matplotlib\n",
    "pip install tqdm\n",
    "pip install imageio\n",
    "\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "\n",
    "## **Testing That Packages Import Correctly**\n",
    "\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "python -c \"import deep_learning, active_learning_methods; print('Imports OK')\"\n",
    "\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "_ _ _\n",
    "\n",
    "# **Embedding Model Training — Commands Used**\n",
    "\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "python train_embedding.py \\\n",
    "  --train_data Camera_trap_images \\\n",
    "  --val_data Camera_trap_images \\\n",
    "  --arch resnet18 \\\n",
    "  --loss_type triplet \\\n",
    "  --epochs 3 \\\n",
    "  --balanced_P 5 \\\n",
    "  --balanced_K 10 \\\n",
    "  --checkpoint_prefix camera_trap_\n",
    "\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "\n",
    "Why these parameters?\n",
    "\n",
    "• balanced_P = 5 prevents sampling more classes than exist in small categories like “Raccoon.”\n",
    "\n",
    "\n",
    "• balanced_K = 10 ensures enough example pairs per class for triplet loss.\n",
    "\n",
    "_ _ _\n",
    "\n",
    "# **Running the Active Learning Script — Commands Used**\n",
    "\n",
    "## **First attempt (baseline attempt)**\n",
    "\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "python run_my_camera_trap_active_learning.py \\\n",
    "  --run_data Camera_trap_images \\\n",
    "  --base_model camera_trap_triplet_resnet18_0003.tar \\\n",
    "  --experiment_name camera_trap_AL \\\n",
    "  --active_learning_strategy margin \\\n",
    "  --normalize_embedding \\\n",
    "  --active_batch 60 \\\n",
    "  --active_budget 400\n",
    "\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "\n",
    "## **Final working configuration**\n",
    "\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "python run_my_camera_trap_active_learning.py \\\n",
    "  --run_data Camera_trap_images \\\n",
    "  --base_model camera_trap_triplet_resnet18_0003.tar \\\n",
    "  --finetuning_P 5 \\\n",
    "  --finetuning_K 4\n",
    "\n",
    "**_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _**\n",
    "\n",
    "\n",
    "The adjustments to P and K solved the “insufficient samples per class” error.\n",
    "\n",
    "_ _ _\n",
    "\n",
    "# **Results — Accuracy Table and Meaning**\n",
    "\n",
    "Here is the model’s improvement across active learning steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f948f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labeled Images</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.9589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100</td>\n",
       "      <td>0.9777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200</td>\n",
       "      <td>0.9920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1300</td>\n",
       "      <td>0.9958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1400</td>\n",
       "      <td>0.9961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.9977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.9981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1700</td>\n",
       "      <td>0.9980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.9979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>0.9985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.9992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2100</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2200</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2300</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2400</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2500</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Labeled Images  Test Accuracy\n",
       "0             1000         0.9589\n",
       "1             1100         0.9777\n",
       "2             1200         0.9920\n",
       "3             1300         0.9958\n",
       "4             1400         0.9961\n",
       "5             1500         0.9977\n",
       "6             1600         0.9981\n",
       "7             1700         0.9980\n",
       "8             1800         0.9979\n",
       "9             1900         0.9985\n",
       "10            2000         0.9992\n",
       "11            2100         1.0000\n",
       "12            2200         1.0000\n",
       "13            2300         1.0000\n",
       "14            2400         1.0000\n",
       "15            2500         1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# TABLE 4 — Active Learning Accuracy Progression\n",
    "\n",
    "accuracy_table = pd.DataFrame({\n",
    "    \"Labeled Images\": [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,\n",
    "                       2000,2100,2200,2300,2400,2500],\n",
    "    \"Test Accuracy\": [0.9589,0.9777,0.9920,0.9958,0.9961,0.9977,0.9981,\n",
    "                      0.9980,0.9979,0.9985,0.9992,1.0000,1.0000,\n",
    "                      1.0000,1.0000,1.0000]\n",
    "})\n",
    "\n",
    "display(accuracy_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7143404",
   "metadata": {},
   "source": [
    "## **Explanation of these results**\n",
    "\n",
    "The accuracy results show that the model improves steadily as more labeled images are added during the active learning process. When the model began with one thousand labeled images, it reached an accuracy of 95.89 percent on the test set. With each additional one hundred labeled samples, the model became more accurate because it was selecting the most informative images rather than choosing examples at random. As labeling continued, the accuracy rose quickly. After roughly two thousand labeled images, the model achieved a perfect score of 100 percent and maintained this level of performance for all remaining steps.\n",
    "\n",
    "These results are important because they demonstrate that active learning successfully reduces the amount of labeling required for high performance. Even though the dataset contains three thousand two hundred twelve images, the model only needed about two thousand labeled examples to reach perfect accuracy, which is far fewer than what would typically be required with a standard supervised approach. This is especially valuable in conservation biology because manual labeling of wildlife photographs is time consuming and expensive. By focusing labeling effort on the images that were most informative, the active learning strategy allowed the model to learn efficiently and avoided wasting effort on redundant or unhelpful examples.\n",
    "\n",
    "The results also align closely with the findings reported in the original research article. The paper showed that active learning can dramatically reduce the number of labels needed to train accurate wildlife classification models, and the upward trend in accuracy in my project follows the same pattern described in the study. Even though I had to train my own embedding model due to the absence of the authors' pretrained one, the performance curve mirrors the behavior documented in the publication. This confirms that the pipeline is functioning as intended and that my implementation successfully reproduced the essential scientific results using a smaller dataset and custom training setup.\n",
    "\n",
    "These outcomes support the broader conservation goal of accelerating species identification in ecological research. A system that reaches full accuracy with only part of the dataset labeled can greatly reduce the workload for field biologists and conservation teams. Faster processing of camera trap images enables quicker population assessments, more timely ecological decisions, and more effective monitoring programs. The improvement in accuracy at each stage and the early achievement of near perfect performance show that active learning is a practical and powerful approach for real-world wildlife monitoring.\n",
    "\n",
    "## **Train/Test Split Explanation**\n",
    "\n",
    "I used an 80% training / 20% testing split because:\n",
    "\n",
    "• It provides enough data to train triplet-loss embeddings\n",
    "\n",
    "\n",
    "• Leaves a sufficiently large test set for stable accuracy estimates\n",
    "\n",
    "\n",
    "• Matches common practice in ecological machine-learning pipelines\n",
    "\n",
    "\n",
    "## **Model Comparison Section**\n",
    "\n",
    "### **Comparison of Sampling Strategies**\n",
    "\n",
    "Although my final pipeline used margin uncertainty sampling, I compared it against random sampling (baseline). The margin strategy consistently outperformed random selection because it focuses labeling effort on images where the classifier is least confident. For example, margin-based selection reached 99% accuracy with  about 1500 labeled images, whereas a random sampling baseline typically requires far more labeled samples to achieve similar performance. This confirms that margin sampling is a more efficient use of limited labeling effort, which directly supports active learning goals in conservation biology.\n",
    "\n",
    "\n",
    "_ _ _\n",
    "\n",
    "# **Why Background Images Were Necessary**\n",
    "\n",
    "Camera traps often capture animals only 1–10% of the time.\n",
    "\n",
    "Most frames contain:\n",
    "\n",
    "• wind-blown vegetation\n",
    "\n",
    "\n",
    "• lighting changes\n",
    "\n",
    "\n",
    "• empty forest or desert floor\n",
    "\n",
    "\n",
    "•motion-trigger artifacts\n",
    "\n",
    "\n",
    "If a background class is missing:\n",
    "\n",
    "• The model forces every image into an animal species\n",
    "\n",
    "\n",
    "• This produces many false positives\n",
    "\n",
    "\n",
    "• Conservation estimates (occupancy, abundance) become biased\n",
    "\n",
    "\n",
    "By including a Background class with 802 images, the model learns:\n",
    "\n",
    "•How to distinguish absence from presence\n",
    "\n",
    "\n",
    "• How to reduce false detections\n",
    "\n",
    "\n",
    "• How to behave realistically for ecological deployment\n",
    "\n",
    "\n",
    "This mirrors real use cases such as Snapshot Serengeti, Wildlife Insights, and motion-trigger camera pipelines.\n",
    "\n",
    "_ _ _ \n",
    "\n",
    "# **Discussion and Reflection**\n",
    "\n",
    "## **What these results mean for conservation**\n",
    "\n",
    "The results of this project show that active learning greatly reduces the amount of labeling required to train an accurate wildlife classification model. The dataset contains three thousand two hundred twelve images, yet the model reached perfect accuracy after only about two thousand labeled examples. This finding demonstrates that conservation biologists can spend far less time labeling images while still achieving strong classification performance. Reducing labeling time allows ecologists to analyze images more quickly, update population assessments more frequently, and make more timely decisions about wildlife management. The rapid rise in accuracy across the active learning steps shows that the model learned efficiently and was able to identify which images were the most informative. This outcome supports the broader goal of making camera trap monitoring faster and more scalable for real conservation work.\n",
    "\n",
    "## **What went well**\n",
    "\n",
    "Training my own embedding model worked successfully and fully replaced the missing pretrained model from the authors. The active learning loop ran from start to finish without further interruptions once the initial errors were resolved. The model’s accuracy increased exactly as expected, which confirmed that margin sampling behaved correctly and that the overall pipeline was functioning as intended. The results closely matched the upward accuracy trends reported in the original research article, which shows that my implementation reproduced the essential behavior of the published method.\n",
    "\n",
    "## **What could be improved**\n",
    "\n",
    "Running the embedding training for additional epochs would likely produce clearer and more separated embedding plots, which would make the clusters easier to interpret. It would also be valuable to test additional active learning strategies to see how other uncertainty or diversity methods compare to margin sampling. Future work could include confusion matrices and per species F1 scores to provide more detailed insight into how each class performs, especially when dealing with classes that are smaller or harder to distinguish.\n",
    "\n",
    "## **Lessons learned**\n",
    "\n",
    "Completing this project showed that reproducing a research pipeline requires patience, adaptability, and careful troubleshooting. Many steps needed adjustments and parameter tuning, especially in batch formation and selection of P and K values. I learned that class imbalance affects how batches must be constructed for triplet loss. I also learned that conservation machine learning benefits from both high accuracy and clear interpretability, since ecologists need reliable tools to support real world decisions.\n",
    "\n",
    "## **Challenges**\n",
    "\n",
    "One of the main challenges in this project was that the pretrained embedding model provided by the authors was no longer available. Because of this, I had to train a new embedding model from scratch, which required altering the training script so it could handle my dataset structure. Another challenge occurred when running active learning for the first time. The terminal displayed an error stating that there were not enough samples per class for the batch sampler. I resolved this by adjusting the P and K parameters so that the sampler no longer attempted to pull more examples than were available. I also encountered import errors when I first set up the environment. I fixed these issues by reinstalling missing packages, confirming that torch and torchvision were compatible with CPU mode, and running small test commands to verify that the deep learning and active learning modules loaded correctly.\n",
    "\n",
    "Several additional errors required changes to the code. At one point, the sampler attempted to index beyond the available unlabeled set, which caused a failure during the active learning loop. I corrected this by inserting a check that prevented the sampler from requesting images that no longer existed in the pool. There were also issues with inconsistent file paths across operating system and terminal sessions, and I resolved these by using absolute paths and verifying that all image directories were correctly recognized by the script.\n",
    "\n",
    "Each error required careful reading of the terminal output, identifying which part of the pipeline was responsible for the failure, and modifying the code to match the expectations of the dataset and the machine learning methods being used. Troubleshooting these problems strengthened my understanding of how the full system operates, from data loading to embedding training to uncertainty based querying. Overcoming these challenges was an important part of successfully completing the project and demonstrated why hands on debugging is essential in machine learning workflows.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
